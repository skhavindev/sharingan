# ğŸ‘ï¸ Sharingan - Semantic Video Understanding

**Sharingan** is a lightweight Python library for semantic video understanding with temporal reasoning. It combines vision-language models (CLIP, SmolVLM) with temporal analysis to understand video content at a deep semantic level.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸ¬ **Semantic Video Processing** - Understand video content beyond pixels
- ğŸ” **Natural Language Queries** - Search videos using text descriptions
- ğŸ¤– **AI Chat** - Conversational interface with Qwen2.5-0.5B
- âš¡ **Temporal Reasoning** - Cross-frame attention and memory tokens
- ğŸ¯ **Event Detection** - Automatically identify key moments
- ğŸ’¾ **Efficient Storage** - 130x compression with Int8 quantizations
- ğŸŒ **Web UI** - Beautiful Flask-based interface
- ğŸš€ **Fast Processing** - Batch processing and GPU acceleration

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Sharingan Pipeline                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Video Input                                                â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Frame Samplerâ”‚  (Adaptive FPS)                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Vision-Language Models         â”‚                       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                       â”‚
â”‚  â”‚  â”‚  CLIP  â”‚  or  â”‚ SmolVLM-500Mâ”‚ â”‚                       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                       â”‚
â”‚  â”‚   (Fast)         (Detailed)      â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚             â”‚                                               â”‚
â”‚             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Temporal Reasoning Engine      â”‚                       â”‚
â”‚  â”‚  â€¢ Cross-Frame Gating            â”‚                       â”‚
â”‚  â”‚  â€¢ Memory Tokens                 â”‚                       â”‚
â”‚  â”‚  â€¢ Temporal Attention            â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚             â”‚                                               â”‚
â”‚             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Embedding Storage (Int8)       â”‚                       â”‚
â”‚  â”‚   ~2.3MB for 5-min video         â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚             â”‚                                               â”‚
â”‚             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚             â–¼          â–¼              â–¼                     â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚      â”‚  Events  â”‚ â”‚ Queries â”‚  â”‚ AI Chat  â”‚                 â”‚
â”‚      â”‚ Detector â”‚ â”‚  (CLIP) â”‚  â”‚ (Qwen2.5)â”‚                 â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Installation

```bash
pip install sharingan-core

# Optional: For GPU acceleration
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Optional: For AI chat
pip install transformers bitsandbytes accelerate
```

### Basic Usage

```python
from sharingan import VideoProcessor

# Process a video
processor = VideoProcessor(
    vlm_model='clip',  # or 'smolvlm' for detailed descriptions
    device='auto'
)

results = processor.process('video.mp4')

# Query the video
matches = processor.query('person speaking')
for match in matches:
    print(f"Found at {match.timestamp}s - {match.confidence:.2%}")

# AI Chat (optional)
response = processor.chat('What happens in this video?')
print(response)
```

### Web UI

```bash
# Launch the web interface
python -m sharingan.cli ui

# Or programmatically
from sharingan.ui import run_ui
run_ui(port=5000, open_browser=True)
```

## ğŸ“– Documentation

### Vision Models

**CLIP (Default)**
- Fast semantic embeddings
- Best for: Quick search, real-time processing
- Memory: ~400MB

**SmolVLM-500M**
- Detailed frame descriptions
- Best for: Rich understanding, detailed analysis
- Memory: ~538MB (8-bit quantized)

### Processing Options

```python
processor = VideoProcessor(
    vlm_model='clip',           # 'clip' or 'smolvlm'
    device='auto',              # 'cpu', 'cuda', or 'auto'
    target_fps=5.0,             # Frames per second to process
    enable_temporal=True,       # Temporal reasoning
    enable_tracking=False       # Entity tracking
)
```

### Query Options

```python
# Semantic search
results = processor.query(
    'person speaking',
    top_k=5
)

# AI chat (requires Qwen2.5)
response = processor.chat(
    'Describe the main events',
    use_llm=True
)
```

## ğŸ¯ Use Cases

- **Video Search** - Find specific moments using natural language
- **Content Moderation** - Detect inappropriate content
- **Video Summarization** - Generate automatic summaries
- **Accessibility** - Create descriptions for visually impaired
- **Research** - Analyze video datasets at scale

## ğŸ”§ Advanced Features

### Temporal Reasoning

Sharingan uses advanced temporal reasoning:
- **Cross-Frame Gating** - Learns which frames are important
- **Memory Tokens** - Maintains context across the video
- **Temporal Attention** - Understands relationships between frames

### Efficient Storage

Videos are compressed 130x using Int8 quantization:
- 5-minute video: ~2.3MB (vs 300MB raw)
- Fast cache loading
- No quality loss for search

### Event Detection

Automatically detects:
- Scene changes
- Motion patterns
- Content transitions

## ğŸ“Š Performance

| Model | Speed | Memory | Quality |
|-------|-------|--------|---------|
| CLIP | âš¡âš¡âš¡ | 400MB | Good |
| SmolVLM | âš¡âš¡ | 538MB | Excellent |

*Tested on NVIDIA RTX 3060 (4GB VRAM)*

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [OpenAI CLIP](https://github.com/openai/CLIP)
- [SmolVLM](https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct)
- [Qwen2.5](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)

## ğŸ“§ Contact

For questions and support, please open an issue on GitHub.

---

Made with â˜•